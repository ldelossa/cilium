//  Copyright (C) Isovalent, Inc. - All Rights Reserved.
//
//  NOTICE: All information contained herein is, and remains the property of
//  Isovalent Inc and its suppliers, if any. The intellectual and technical
//  concepts contained herein are proprietary to Isovalent Inc and its suppliers
//  and may be covered by U.S. and Foreign Patents, patents in process, and are
//  protected by trade secret or copyright law.  Dissemination of this information
//  or reproduction of this material is strictly forbidden unless prior written
//  permission is obtained from Isovalent Inc.

package srv6manager

import (
	"context"
	"errors"
	"fmt"
	"net"

	"github.com/cilium/hive/cell"
	"github.com/cilium/hive/job"
	"github.com/cilium/stream"
	"github.com/sirupsen/logrus"
	"go4.org/netipx"
	"k8s.io/apimachinery/pkg/types"

	"github.com/cilium/cilium/enterprise/pkg/srv6/sidmanager"
	srv6Types "github.com/cilium/cilium/enterprise/pkg/srv6/types"
	"github.com/cilium/cilium/pkg/bgpv1/agent/signaler"
	"github.com/cilium/cilium/pkg/ebpf"
	"github.com/cilium/cilium/pkg/identity"
	identityCache "github.com/cilium/cilium/pkg/identity/cache"
	"github.com/cilium/cilium/pkg/ipam"
	iso_v1alpha1 "github.com/cilium/cilium/pkg/k8s/apis/isovalent.com/v1alpha1"
	"github.com/cilium/cilium/pkg/k8s/resource"
	k8sTypes "github.com/cilium/cilium/pkg/k8s/types"
	"github.com/cilium/cilium/pkg/labels"
	"github.com/cilium/cilium/pkg/lock"
	"github.com/cilium/cilium/pkg/logging/logfields"
	"github.com/cilium/cilium/pkg/maps/srv6map"
	"github.com/cilium/cilium/pkg/option"
	"github.com/cilium/cilium/pkg/promise"
	"github.com/cilium/cilium/pkg/time"
)

const (
	ownerName = "srv6-manager"
)

var (
	legacySIDStructure = srv6Types.MustNewSIDStructure(128, 0, 0, 0)
)

// SIDAllocation is a bookkeeping structure for locally allocated SIDs.
// These SID allocations serve as SRV6 VRF locators.
type SIDAllocation struct {
	VRFID       uint32
	SIDInfo     *sidmanager.SIDInfo
	LocatorPool string
}

// The SRv6 manager stores the internal data to track SRv6 policies, VRFs,
// and SIDs. It also hooks up all the callbacks to update the BPF SRv6 maps
// accordingly.
//
// The SRv6 manager is capable of notifying the BGP Control Plane when changes
// to its internal databases occur.
type Manager struct {
	lock.RWMutex

	logger logrus.FieldLogger

	// policies stores egress policies indexed by policyID
	policies map[policyID]*EgressPolicy

	// vrfs stores VRFs indexed by vrfID
	vrfs map[vrfID]*VRF

	// cepResource provides access to events and read-only store of CiliumEndpoint resources
	cepResource resource.Resource[*k8sTypes.CiliumEndpoint]

	// vrfResource provides access to events and read-only store of IsovalentVRF resources
	vrfResource resource.Resource[*iso_v1alpha1.IsovalentVRF]

	// policyResource provides access to events and read-only store of IsovalentSRv6EgressPolicy resources
	policyResource resource.Resource[*iso_v1alpha1.IsovalentSRv6EgressPolicy]

	// cepStore is a read-only store of CiliumEndpoint resources
	cepStore resource.Store[*k8sTypes.CiliumEndpoint]

	// identityAllocator is used to fetch identity labels for endpoint updates
	identityAllocator identityCache.IdentityAllocator

	// allocatedSIDs map VRF IDs to their allocated SID.
	//
	// When we encounter VRFs, a SID is allocated locally and stored in this map.
	// The map is then referenced to determine if SID allocation/deallocation is
	// necessary on VRF reconciliation.
	allocatedSIDs map[uint32]*SIDAllocation

	// BGP Control Plane signaler
	bgp *signaler.BGPCPSignaler

	// sidAlloc is an IPv6Allocator used to allocate L3VPN service SID's on VRF
	// creation.
	sidAlloc ipam.Allocator

	// Promise to wait for the initialization of the Daemon. This is solely used
	// for waiting for the IPAM initialization. Once the IPAM subsystem becomes
	// modular, we can change this to inject IPAM cell.
	daemonPromise promise.Promise[daemon]

	// sidManager is an interface to interact with SIDManager
	sidManagerPromise promise.Promise[sidmanager.SIDManager]
	sidManager        sidmanager.SIDManager

	// Local copy of the SIDAllocator generated by the SIDManager. The key
	// is the name of the locator pool and the value is the SIDAllocator.
	// Manager.RWMutex must be held when accessing.
	sidAllocatorSyncers map[string]sidmanager.SIDAllocator

	// BPF Maps
	vrfMap4    *srv6map.VRFMap4
	vrfMap6    *srv6map.VRFMap6
	policyMap4 *srv6map.PolicyMap4
	policyMap6 *srv6map.PolicyMap6
	sidMap     *srv6map.SIDMap
}

type Params struct {
	cell.In

	JobGroup                  job.Group
	Logger                    logrus.FieldLogger
	DaemonConfig              *option.DaemonConfig
	Sig                       *signaler.BGPCPSignaler
	CacheIdentityAllocator    identityCache.IdentityAllocator
	DaemonPromise             promise.Promise[daemon]
	SIDManagerPromise         promise.Promise[sidmanager.SIDManager]
	CiliumEndpointResource    resource.Resource[*k8sTypes.CiliumEndpoint]
	IsovalentVRFResource      resource.Resource[*iso_v1alpha1.IsovalentVRF]
	IsovalentSRv6EgressPolicy resource.Resource[*iso_v1alpha1.IsovalentSRv6EgressPolicy]
	VRF4Map                   *srv6map.VRFMap4
	VRF6Map                   *srv6map.VRFMap6
	Policy4Map                *srv6map.PolicyMap4
	Policy6Map                *srv6map.PolicyMap6
	SIDMap                    *srv6map.SIDMap
}

// NewSRv6Manager returns a new SRv6 policy manager.
func NewSRv6Manager(p Params) *Manager {
	if !p.DaemonConfig.EnableSRv6 {
		return nil
	}

	manager := &Manager{
		logger:              p.Logger,
		policies:            make(map[policyID]*EgressPolicy),
		vrfs:                make(map[vrfID]*VRF),
		identityAllocator:   p.CacheIdentityAllocator,
		allocatedSIDs:       make(map[uint32]*SIDAllocation),
		bgp:                 p.Sig,
		daemonPromise:       p.DaemonPromise,
		sidManagerPromise:   p.SIDManagerPromise,
		cepResource:         p.CiliumEndpointResource,
		vrfResource:         p.IsovalentVRFResource,
		policyResource:      p.IsovalentSRv6EgressPolicy,
		sidAllocatorSyncers: make(map[string]sidmanager.SIDAllocator),
		vrfMap4:             p.VRF4Map,
		vrfMap6:             p.VRF6Map,
		policyMap4:          p.Policy4Map,
		policyMap6:          p.Policy6Map,
		sidMap:              p.SIDMap,
	}

	initDone := make(chan struct{})
	vrfSyncDone := make(chan struct{})

	p.JobGroup.Add(
		job.OneShot("init", func(ctx context.Context, health cell.Health) error {
			// Wait for the IPAM to be initialized
			d, err := manager.daemonPromise.Await(ctx)
			if err != nil {
				return fmt.Errorf("failed to await on Daemon (IPAM): %w", err)
			}
			manager.setSIDAllocator(d.GetIPv6Allocator())

			// Create Endpoints store and watch for Endpoints events
			manager.cepStore, err = manager.cepResource.Store(ctx)
			if err != nil {
				return fmt.Errorf("failed creating Endpoints resource.Store: %w", err)
			}

			close(initDone)

			return nil
		}, job.WithRetry(100, &job.ExponentialBackoff{Min: 100 * time.Millisecond, Max: time.Second})),

		job.OneShot("cep-watcher", func(ctx context.Context, health cell.Health) error {
			select {
			case <-initDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			select {
			case <-vrfSyncDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			epSynced := false
			for event := range manager.cepResource.Events(ctx) {
				// reconcile upon CiliumEndpoint events
				manager.Lock()
				switch event.Kind {
				case resource.Sync:
					epSynced = true
					manager.reconcileVRF()
				case resource.Upsert, resource.Delete:
					if epSynced {
						manager.reconcileVRF()
					}
				}
				manager.Unlock()
				event.Done(nil)
			}

			return ctx.Err()
		}),

		job.OneShot("vrf-watcher", func(ctx context.Context, health cell.Health) error {
			select {
			case <-initDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			for event := range manager.vrfResource.Events(ctx) {
				// reconcile upon IsovalentVRF events
				switch event.Kind {
				case resource.Sync:
					close(vrfSyncDone)
				case resource.Upsert:
					vrf, err := ParseVRF(event.Object)
					if err != nil {
						event.Done(err)
						continue
					}
					manager.OnAddSRv6VRF(*vrf)
				case resource.Delete:
					manager.OnDeleteSRv6VRF(vrfID{
						Namespace: event.Key.Namespace,
						Name:      event.Key.Name,
					})
				}
				event.Done(nil)
			}

			return ctx.Err()
		}),

		job.OneShot("policy-watcher", func(ctx context.Context, health cell.Health) error {
			select {
			case <-initDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			select {
			case <-vrfSyncDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			for event := range manager.policyResource.Events(ctx) {
				// reconcile upon IsovalentSRv6EgressPolicy events
				switch event.Kind {
				case resource.Upsert:
					policy, err := ParsePolicy(event.Object)
					if err != nil {
						event.Done(err)
						continue
					}
					manager.OnAddSRv6Policy(*policy)
				case resource.Delete:
					manager.OnDeleteSRv6Policy(policyID{
						Namespace: event.Key.Namespace,
						Name:      event.Key.Name,
					})
				}
				event.Done(nil)
			}

			return ctx.Err()
		}),

		job.OneShot("sidmanager-watcher", func(ctx context.Context, health cell.Health) error {
			select {
			case <-initDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			// SIDManager's Start hook should already called and initial sync is
			// running. So, it's safe to wait on it here.
			sidManager, err := manager.sidManagerPromise.Await(ctx)
			if err != nil {
				return fmt.Errorf("failed to await on SIDManager: %w", err)
			}

			// Wait for an initial k8s resource sync. This is required for
			// VRF SID restoration after agent restart. When we subscribe
			// to the SID manager, it calls OnAddLocator for all existing
			// locators. Within the callback, SRv6Manager must retrieve an
			// existing SID allocation for all VRFs. Thus, at that point,
			// all VRFs must be synced.
			//
			// This needs to be asynchronous and we shouldn't block within
			// SRv6Manager's Start hook since the Daemon depends on
			// SRv6Manager to register it to k8s VRF event handler and it
			// is called after this Start hook.
			select {
			case <-vrfSyncDone:
			case <-ctx.Done():
				return ctx.Err()
			}

			// Subscribe to the SIDManager
			for ev := range stream.ToChannel[sidmanager.Event](ctx, sidManager) {
				switch ev.Kind {
				case sidmanager.Sync:
					manager.Lock()
					// Now we can set sidManager since the initial sync is done.
					// This makes sidAllocatorIsSet to return true. From now on,
					// VRF k8s event handler can allocate SIDs from SIDManager.
					manager.sidManager = sidManager
					// SIDManager-related initialization is done. Kick an
					// initial VRF reconciliation.
					manager.reconcileVRF()
					manager.Unlock()

				case sidmanager.Upsert:
					manager.Lock()

					// Locator pool is added or updated
					oldAllocator, found := manager.sidAllocatorSyncers[ev.PoolName]
					if !found {
						// Do initial allocation. This includes
						// restoring existing allocations from
						// the previous agent run.
						manager.onAddLocator(ev.PoolName, ev.Allocator)
					} else {
						// Release existing allocations
						// associated with the updated pool and
						// allocate new SIDs.
						manager.onUpdateLocator(ev.PoolName, oldAllocator, ev.Allocator)
					}

					// Update the reference
					manager.sidAllocatorSyncers[ev.PoolName] = ev.Allocator

					// Reconcile VRF
					manager.reconcileVRF()

					manager.Unlock()

				case sidmanager.Delete:
					manager.Lock()

					// Locator pool is deleted
					oldAllocator, found := manager.sidAllocatorSyncers[ev.PoolName]
					if found {
						// Delete existing allocations associated with the deleted pool
						manager.onDeleteLocator(ev.PoolName, oldAllocator)
						delete(manager.sidAllocatorSyncers, ev.PoolName)
					}

					// Reconcile VRF
					manager.reconcileVRF()

					manager.Unlock()
				}
			}

			return ctx.Err()
		}),
	)

	return manager
}

func (manager *Manager) updateVRFSIDAllocation(vrf *VRF, pool string, newInfo *sidmanager.SIDInfo) {
	vrf.SIDInfo = newInfo
	manager.allocatedSIDs[vrf.VRFID] = &SIDAllocation{
		VRFID:       vrf.VRFID,
		SIDInfo:     newInfo,
		LocatorPool: pool,
	}
}

func (manager *Manager) deleteVRFSIDAllocation(vrfID uint32) {
	delete(manager.allocatedSIDs, vrfID)
}

func (manager *Manager) restoreExistingAllocations(pool string, allocator sidmanager.SIDAllocator) {
	for _, info := range allocator.AllocatedSIDs(ownerName) {
		// Find VRF associated with this SID from allocation metadata
		if vrf, exists := manager.vrfs[types.NamespacedName{Name: info.MetaData}]; !exists {
			// SID is allocated, but associated VRF doesn't exist.
			// One possible case is users delete the VRF while
			// Cilium is stopping. Release the SID to align with an
			// actual state.
			if err := allocator.Release(info.SID.Addr); err != nil {
				manager.logger.WithError(err).Warn("Failed to release stale SID")
			} else {
				manager.logger.WithFields(logrus.Fields{
					logfields.VRF: info.MetaData,
					logfields.SID: info.SID.String(),
				}).Debug("Released stale SID")
			}
		} else {
			// This VRF is not interested in this pool anymore.
			// Release existing SID allocation. This happens when
			// users modifiy the LocatorPool while Cilium is
			// stopping.
			if vrf.LocatorPool != pool {
				if err := allocator.Release(info.SID.Addr); err != nil {
					manager.logger.WithError(err).Warn("Failed to release stale SID")
					continue
				} else {
					manager.logger.WithFields(logrus.Fields{
						logfields.VRF: info.MetaData,
						logfields.SID: info.SID.String(),
					}).Debug("Released stale SID")
				}
			} else {
				if vrf.SIDInfo != nil {
					// SID is already allocated for this
					// VRF. This happens when there's two
					// or more SID bounded to this VRF.
					// Currently, we suppose to have only
					// one SID for each VRF, so this is a
					// bug. The desired state here is
					// there's only one SID allocated for
					// each VRF, so release the SID which
					// we are seeing now.
					if err := allocator.Release(info.SID.Addr); err != nil {
						manager.logger.WithError(err).Warn("Failed to release stale SID")
						continue
					}

					manager.logger.WithFields(logrus.Fields{
						logfields.VRF: info.MetaData,
						logfields.SID: info.SID.String(),
					}).Warn("More than one SID allocation for the VRF observed. Releasing unnecessary allocation.")
				} else {
					// Restore SID
					vrf.SIDInfo = info

					manager.allocatedSIDs[vrf.VRFID] = &SIDAllocation{
						VRFID:       vrf.VRFID,
						SIDInfo:     info,
						LocatorPool: pool,
					}

					manager.logger.WithFields(logrus.Fields{
						logfields.VRF: info.MetaData,
						logfields.SID: info.SID.String(),
					}).Debug("Restored SID allocation")
				}
			}
		}
	}
}

func (manager *Manager) onAddLocator(pool string, allocator sidmanager.SIDAllocator) {
	// Restore an existing allocation from pool. This is only called in the
	// context of an initial Subscribe() after agent restart.
	if manager.sidManager == nil {
		manager.restoreExistingAllocations(pool, allocator)
	}

	// Iterate over all existing VRFs and allocate SID if missing
	for id, vrf := range manager.vrfs {
		// This VRF is not interested in this pool
		if vrf.LocatorPool != pool {
			continue
		}

		// Allocation already exists
		if vrf.SIDInfo != nil {
			continue
		}

		info, err := allocator.AllocateNext(ownerName, id.Name, manager.selectBehavior(allocator.BehaviorType()))
		if err != nil {
			manager.logger.WithError(err).Error("Failed to allocate SID")
			continue
		}

		manager.updateVRFSIDAllocation(vrf, pool, info)
	}
}

func (manager *Manager) onUpdateLocator(pool string, oldAllocator, newAllocator sidmanager.SIDAllocator) {
	// Update all existing allocations associated with this pool
	for id, vrf := range manager.vrfs {
		if vrf.LocatorPool != pool {
			continue
		}

		if vrf.SIDInfo == nil {
			continue
		}

		var (
			err  error
			info *sidmanager.SIDInfo
		)

		behavior := manager.selectBehavior(newAllocator.BehaviorType())

		// Try to allocate the same SID first to reduce BGP update churn
		info, err = newAllocator.Allocate(vrf.SIDInfo.SID.Addr, ownerName, id.Name, behavior)
		if err != nil {
			// Failed to allocate the same SID from new pool. This
			// is still ok. Allocate a brand new SID.
			info, err = newAllocator.AllocateNext(ownerName, id.Name, behavior)
			if err != nil {
				manager.logger.WithError(err).Error("Failed to allocate SID")
				continue
			}
		}

		manager.deleteVRFSIDAllocation(vrf.VRFID)
		manager.updateVRFSIDAllocation(vrf, pool, info)
	}
}

func (manager *Manager) onDeleteLocator(pool string, allocator sidmanager.SIDAllocator) {
	// Delete all existing allocations associated with this pool
	for _, vrf := range manager.vrfs {
		if vrf.LocatorPool != pool {
			continue
		}

		if vrf.SIDInfo == nil {
			continue
		}

		if err := allocator.Release(vrf.SIDInfo.SID.Addr); err != nil {
			manager.logger.WithError(err).Error("Failed to release SID allocated from deleted pool")
		}

		vrf.SIDInfo = nil
		manager.deleteVRFSIDAllocation(vrf.VRFID)
	}
}

func (manager *Manager) setSIDAllocator(a ipam.Allocator) {
	manager.Lock()
	defer manager.Unlock()
	manager.sidAlloc = a
}

func (manager *Manager) sidAllocatorIsSet() bool {
	return manager.sidAlloc != nil && manager.sidManager != nil
}

// GetAllVRFs returns a slice with all copy of VRFs known to the SRv6 manager.
func (manager *Manager) GetAllVRFs() []*VRF {
	manager.RLock()
	defer manager.RUnlock()

	vrfs := make([]*VRF, 0, len(manager.vrfs))
	for _, vrf := range manager.vrfs {
		vrfs = append(vrfs, vrf.DeepCopy())
	}
	return vrfs
}

// GetVRFs returns a slice with copy of VRFs known to the SRv6 manager that
// have the given import route-target.
func (manager *Manager) GetVRFs(importRouteTarget string) []*VRF {
	manager.RLock()
	defer manager.RUnlock()

	vrfs := make([]*VRF, 0, len(manager.vrfs))
	for _, vrf := range manager.vrfs {
		if vrf.ImportRouteTarget == importRouteTarget {
			vrfs = append(vrfs, vrf.DeepCopy())
		}
	}
	return vrfs
}

// GetVRFByName returns a copy of the VRF with the given VRF name known to the SRv6 manager.
func (manager *Manager) GetVRFByName(vrfName types.NamespacedName) (*VRF, bool) {
	manager.RLock()
	defer manager.RUnlock()

	vrf, exists := manager.vrfs[vrfName]
	if !exists {
		return nil, false
	}
	return vrf.DeepCopy(), true
}

// GetEgressPolicies returns a slice with the SRv6 egress policies known to the
// SRv6 manager.
func (manager *Manager) GetEgressPolicies() []*EgressPolicy {
	manager.RLock()
	defer manager.RUnlock()

	policies := make([]*EgressPolicy, 0, len(manager.policies))
	for _, policy := range manager.policies {
		policies = append(policies, policy.DeepCopy())
	}
	return policies
}

// Event handlers

// OnAddSRv6Policy and updates the manager internal state with the policy
// fields.
func (manager *Manager) OnAddSRv6Policy(policy EgressPolicy) {
	manager.Lock()
	defer manager.Unlock()

	logger := manager.logger.WithField(logfields.IsovalentSRv6EgressPolicyName, policy.id.Name)

	if _, ok := manager.policies[policy.id]; !ok {
		logger.Info("Added IsovalentSRv6EgressPolicy")
	} else {
		logger.Info("Updated IsovalentSRv6EgressPolicy")
	}

	manager.policies[policy.id] = &policy

	manager.reconcilePoliciesAndSIDs()
}

// OnDeleteSRv6Policy deletes the internal state associated with the given
// policy.
func (manager *Manager) OnDeleteSRv6Policy(policyID policyID) {
	manager.Lock()
	defer manager.Unlock()

	logger := manager.logger.WithField(logfields.IsovalentSRv6EgressPolicyName, policyID.Name)

	if manager.policies[policyID] == nil {
		logger.Warn("Can't delete IsovalentSRv6EgressPolicy: policy not found")
		return
	}

	logger.Info("Deleted IsovalentSRv6EgressPolicy")

	delete(manager.policies, policyID)

	manager.reconcilePoliciesAndSIDs()
}

// OnAddSRv6VRF and updates the manager internal state with the VRF
// config fields.
func (manager *Manager) OnAddSRv6VRF(vrf VRF) {
	manager.Lock()
	defer manager.Unlock()

	logger := manager.logger.WithField(logfields.IsovalentVRFName, vrf.id.Name)

	if _, ok := manager.vrfs[vrf.id]; !ok {
		logger.Info("Added IsovalentVRF")
	} else {
		logger.Info("Updated IsovalentVRF")
	}

	manager.vrfs[vrf.id] = &vrf

	manager.reconcileVRF()
}

// OnDeleteSRv6VRF deletes the internal state associated with the given VRF.
func (manager *Manager) OnDeleteSRv6VRF(vrfID vrfID) {
	manager.Lock()
	defer manager.Unlock()

	logger := manager.logger.WithField(logfields.IsovalentVRFName, vrfID.Name)

	if manager.vrfs[vrfID] == nil {
		logger.Warn("Can't delete IsovalentVRF: policy not found")
		return
	}

	logger.Info("Deleted IsovalentVRF")

	delete(manager.vrfs, vrfID)

	manager.reconcileVRF()
}

// getIdentityLabels waits for the global identities to be populated to the cache,
// then looks up identity by ID from the cached identity allocator and return its labels.
func (manager *Manager) getIdentityLabels(securityIdentity uint32) (labels.Labels, error) {
	identityCtx, cancel := context.WithTimeout(context.Background(), option.Config.KVstoreConnectivityTimeout)
	defer cancel()
	if err := manager.identityAllocator.WaitForInitialGlobalIdentities(identityCtx); err != nil {
		return nil, fmt.Errorf("failed to wait for initial global identities: %w", err)
	}

	identity := manager.identityAllocator.LookupIdentityByID(identityCtx, identity.NumericIdentity(securityIdentity))
	if identity == nil {
		return nil, fmt.Errorf("identity %d not found", securityIdentity)
	}
	return identity.Labels, nil
}

// addMissingSRv6PolicyRules is responsible for adding any missing egress SRv6
// policies stored in the manager (i.e. k8s IsovalentSRv6EgressPolicies) to the
// egress policy BPF map.
func (manager *Manager) addMissingSRv6PolicyRules() {
	srv6Policies := map[srv6map.PolicyKey]srv6map.PolicyValue{}
	manager.policyMap4.IterateWithCallback(
		func(key *srv6map.PolicyKey, val *srv6map.PolicyValue) {
			srv6Policies[*key] = *val
		})
	manager.policyMap6.IterateWithCallback(
		func(key *srv6map.PolicyKey, val *srv6map.PolicyValue) {
			srv6Policies[*key] = *val
		})

	var err error
	for _, policy := range manager.policies {
		for _, dstCIDR := range policy.DstCIDRs {
			policyKey := srv6map.PolicyKey{
				VRFID:    policy.VRFID,
				DestCIDR: dstCIDR,
			}

			policyVal, policyPresent := srv6Policies[policyKey]
			if policyPresent && policyVal.SID == policy.SID {
				continue
			}

			if dstCIDR.Addr().Is4() {
				err = manager.policyMap4.Update(&policyKey, policy.SID)
			} else {
				err = manager.policyMap6.Update(&policyKey, policy.SID)
			}

			logger := manager.logger.WithFields(logrus.Fields{
				logfields.VRF:             policy.VRFID,
				logfields.DestinationCIDR: dstCIDR,
				logfields.SID:             policy.SID,
			})
			if err != nil {
				logger.WithError(err).Error("Error applying egress SRv6 policy")
			} else {
				logger.Info("Egress SRv6 policy applied")
			}
		}
	}
}

// removeUnusedSRv6PolicyRules is responsible for removing any entry in the SRv6 policy BPF map which
// is not baked by an actual k8s IsovalentSRv6EgressPolicy.
//
// The algorithm for this function can be expressed as:
//
//	nextPolicyKey:
//	for each entry in the srv6_policy map {
//	    for each policy in k8s IsovalentSRv6EgressPolices {
//	        if policy matches entry {
//	            // we found one k8s policy that matches the current BPF entry, move to the next one
//	            continue nextPolicyKey
//	        }
//	    }
//
//	    // the current BPF entry is not backed by any k8s policy, delete it
//	    srv6map.RemoveSRv6Policy(entry)
//	}
func (manager *Manager) removeUnusedSRv6PolicyRules() {
	srv6Policies := map[srv6map.PolicyKey]srv6map.PolicyValue{}
	manager.policyMap4.IterateWithCallback(
		func(key *srv6map.PolicyKey, val *srv6map.PolicyValue) {
			srv6Policies[*key] = *val
		})
	manager.policyMap6.IterateWithCallback(
		func(key *srv6map.PolicyKey, val *srv6map.PolicyValue) {
			srv6Policies[*key] = *val
		})

nextPolicyKey:
	for policyKey := range srv6Policies {
		for _, policy := range manager.policies {
			for _, dstCIDR := range policy.DstCIDRs {
				k := srv6map.PolicyKey{
					VRFID:    policy.VRFID,
					DestCIDR: dstCIDR,
				}
				if policyKey.Equal(&k) {
					continue nextPolicyKey
				}
			}
		}

		logger := manager.logger.WithFields(logrus.Fields{
			logfields.VRF:             policyKey.VRFID,
			logfields.DestinationCIDR: policyKey.DestCIDR,
		})

		var err error
		if policyKey.DestCIDR.Addr().Is4() {
			err = manager.policyMap4.Delete(&policyKey)
		} else {
			err = manager.policyMap6.Delete(&policyKey)
		}

		if err != nil {
			logger.WithError(err).Error("Error removing SRv6 egress policy")
		} else {
			logger.Info("SRv6 egress policy removed")
		}
	}
}

func (manager *Manager) addMissingSRv6SIDs() {
	for _, vrf := range manager.vrfs {
		if vrf.SIDInfo == nil {
			continue
		}
		if err := manager.sidMap.Update(&srv6map.SIDKey{SID: vrf.SIDInfo.SID.As16()}, vrf.VRFID); err != nil {
			manager.logger.WithField("VRF", vrf.id.Name).WithError(err).Error("VRF has SID allocation and SIDMap entry is missing, but failed to update")
			continue
		}
	}
}

// removeUnusedSRv6SIDs implements the same as removeUnusedSRv6PolicyRules but
// for the SID map.
func (manager *Manager) removeUnusedSRv6SIDs() {
	srv6SIDs := map[srv6map.SIDKey]srv6map.SIDValue{}
	manager.sidMap.IterateWithCallback(
		func(key *srv6map.SIDKey, val *srv6map.SIDValue) {
			srv6SIDs[*key] = *val
		})

nextSIDKey:
	for sidKey := range srv6SIDs {
		for _, allocation := range manager.allocatedSIDs {
			if sidKey.SID.Addr() == allocation.SIDInfo.SID.Addr {
				continue nextSIDKey
			}
		}

		logger := manager.logger.WithFields(logrus.Fields{
			logfields.SID: sidKey.SID,
		})

		if err := manager.sidMap.Delete(&sidKey); err != nil {
			logger.WithError(err).Error("Error removing SID")
		} else {
			logger.Info("SID removed")
		}
	}
}

// reconcileVRFEgressPath will add and remove mappings from the SRv6VRF
// maps given the current Manager's VRF database.
//
// A VRF is expanded into one or more VRFKey structures which act as keys and
// map to the VRF's ID if the VRF's endpoint selector matches an endpoint's
// label.
//
// The manager keeps a database of known endpoints to compare VRF selection against.
func (m *Manager) reconcileVRFEgressPath() {
	type keyEntry struct {
		key   *srv6map.VRFKey
		value *srv6map.VRFValue
	}
	var (
		l = m.logger.WithFields(
			logrus.Fields{
				"component": "srv6.Manager.reconcileVRFEgressPath",
			},
		)
		srv6VRFs = map[srv6map.VRFKey]keyEntry{}
	)

	m.logger.Info("Reconciling egress datapath for encapsulation.")

	// populate srv6VRFs map
	m.vrfMap4.IterateWithCallback(
		func(key *srv6map.VRFKey, val *srv6map.VRFValue) {
			srv6VRFs[*key] = keyEntry{
				key:   key,
				value: val,
			}
		})
	m.vrfMap6.IterateWithCallback(
		func(key *srv6map.VRFKey, val *srv6map.VRFValue) {
			srv6VRFs[*key] = keyEntry{
				key:   key,
				value: val,
			}
		})

	for _, vrf := range m.vrfs {
		keys := m.getVRFKeysFromMatchingEndpoint(vrf)
		for _, key := range keys {
			vrfVal, vrfPresent := srv6VRFs[key]
			if vrfPresent && vrfVal.value.ID == vrf.VRFID {
				continue
			}
			logger := l.WithFields(logrus.Fields{
				logfields.SourceIP:        key.SourceIP,
				logfields.DestinationCIDR: key.DestCIDR,
				logfields.VRF:             vrf.VRFID,
			})

			var err error
			if key.DestCIDR.Addr().Is4() {
				err = m.vrfMap4.Update(&key, vrf.VRFID)
			} else {
				err = m.vrfMap6.Update(&key, vrf.VRFID)
			}

			if err != nil {
				logger.WithError(err).Error("Error applying SRv6 VRF mapping")
			} else {
				logger.Info("SRv6 VRF mapping applied")
			}
		}
	}

	// remove any existing VRF entries
nextVRFKey:
	for _, vrfKeyEntry := range srv6VRFs {
		for _, vrf := range m.vrfs {
			keys := m.getVRFKeysFromMatchingEndpoint(vrf)
			for _, key := range keys {
				k := srv6map.VRFKey{
					SourceIP: key.SourceIP,
					DestCIDR: key.DestCIDR,
				}
				if vrfKeyEntry.key.Equal(&k) {
					continue nextVRFKey
				}
			}
		}
		logger := l.WithFields(logrus.Fields{
			logfields.SourceIP:        vrfKeyEntry.key.SourceIP,
			logfields.DestinationCIDR: vrfKeyEntry.key.DestCIDR,
		})

		var err error
		if vrfKeyEntry.key.DestCIDR.Addr().Is4() {
			err = m.vrfMap4.Delete(vrfKeyEntry.key)
		} else {
			err = m.vrfMap6.Delete(vrfKeyEntry.key)
		}

		if err != nil {
			logger.WithError(err).Error("Error removing SRv6 VRF mapping")
		} else {
			logger.Info("SRv6 VRF mapping removed")
		}
	}
}

// When a VRF is defined, we must configure both the Manager
// and the eBPF datapath to process ingress traffic destined to the VRF.
//
// This function will organize the Manager's VRFs and SID allocations and then
// create or remove both according to the Manager's state.
func (m *Manager) reconcileVRFIngressPath() {
	var (
		l = m.logger.WithFields(
			logrus.Fields{
				"component": "srv6.Manager.reconcileVRFIngressPath",
			},
		)
		toCreate = []*VRF{}
		toUpdate = []*VRF{}
		toRemove = []*SIDAllocation{}
	)

	// By the time we are in this method, the VRF event has been indexed into
	// the manager's VRF field.
	//
	// ATTENTION: A subtlety exists here in that VRF updates from Kubernetes know nothing
	// about locally allocated SIDs and an update event can overwrite the VRF's
	// locally allocated SID. Therefore, this method must also repopulate the
	// SID's Allocated VRF field.
	for _, v := range m.vrfs {
		alloc, hasSID := m.allocatedSIDs[v.VRFID]

		// does this vrf no SID allocation?
		if !hasSID {
			toCreate = append(toCreate, v)
			continue
		}

		// does this VRF have an existing SID allocation?
		if hasSID {
			// SID allocation exists, re-write allocated SID incase an update overwritten it.
			v.SIDInfo = alloc.SIDInfo

			// Locator pool changed. Need SID reallocation.
			if v.LocatorPool != alloc.LocatorPool {
				toUpdate = append(toUpdate, v)
			}
		}
	}
	// if we have any allocated SIDs which do not have associated VRF definitions
	// remove them.
	for vrfID := range m.allocatedSIDs {
		found := false
		for _, vrf := range m.vrfs {
			if vrf.VRFID == vrfID {
				found = true
				break
			}
		}
		if !found {
			toRemove = append(toRemove, m.allocatedSIDs[vrfID])
		}
	}
	l.WithFields(logrus.Fields{
		"toCreate": len(toCreate),
		"toUpdate": len(toUpdate),
		"toRemove": len(toRemove),
	}).Debug("Reconciling ingress VRF mappings for decapsulation.")

	// remove any SIDs in the SID map which we do not have allocations for
	m.removeUnusedSRv6SIDs()

	// add any SIDs which have allocation, but SIDMap entry is missing
	m.addMissingSRv6SIDs()

	m.createIngressPathVRFs(toCreate)
	m.updateIngressPathVRFs(toUpdate)
	m.removeIngressPathVRFs(toRemove)
}

func (m *Manager) selectBehavior(behaviorType srv6Types.BehaviorType) srv6Types.Behavior {
	switch behaviorType {
	case srv6Types.BehaviorTypeBase:
		return srv6Types.BehaviorEndDT4
	case srv6Types.BehaviorTypeUSID:
		return srv6Types.BehaviorUDT4
	default:
		return srv6Types.BehaviorUnknown
	}
}

// allocateSID allocates SID from SIDAllocator. It hides an implementation
// difference between two allocation method we support right now. When the pool
// is an empty string, it allocates SID from legacy IPAM-based allocator and
// otherwise, it allocates SID from SIDManager.
func (m *Manager) allocateSID(pool, metadata string) (*sidmanager.SIDInfo, error) {
	if pool == "" {
		res, err := m.sidAlloc.AllocateNext(ownerName, "")
		if err != nil {
			return nil, err
		}

		addr, ok := netipx.FromStdIP(res.IP)
		if !ok {
			err := fmt.Errorf("failed to convert IP to Addr")
			if releaseErr := m.sidAlloc.Release(res.IP, ""); releaseErr != nil {
				err = errors.Join(err, fmt.Errorf("failed to release SID: %w", releaseErr))
			}
			return nil, err
		}

		sid, err := srv6Types.NewSID(addr)
		if err != nil {
			m.sidAlloc.Release(res.IP, "")
			return nil, fmt.Errorf("failed to create SID: %w", err)
		}

		info := &sidmanager.SIDInfo{
			Owner:     ownerName,
			MetaData:  metadata,
			SID:       sid,
			Structure: legacySIDStructure,
			Behavior:  srv6Types.BehaviorEndDT4,
		}

		return info, nil
	} else {
		allocator, found := m.sidAllocatorSyncers[pool]
		if !found {
			return nil, sidmanager.ErrAllocatorNotFound
		}

		behavior := m.selectBehavior(allocator.BehaviorType())
		if behavior == srv6Types.BehaviorUnknown {
			return nil, fmt.Errorf("unknown behavior")
		}

		info, err := allocator.AllocateNext(ownerName, metadata, behavior)
		if err != nil {
			return nil, err
		}

		return info, nil
	}
}

// releaseSID releases SID from SIDAllocator. It hides an implementation
// difference between two allocation method we support right now. When the pool
// is an empty string, it releases SID with legacy IPAM-based allocator and
// otherwise, it releases SID with SIDManager.
func (m *Manager) releaseSID(pool string, sid srv6Types.SID) error {
	if pool == "" {
		if err := m.sidAlloc.Release(net.IP(sid.Addr.AsSlice()), ""); err != nil {
			return err
		}
		return nil
	} else {
		allocator, found := m.sidAllocatorSyncers[pool]
		if !found {
			// We want to release the SID and the allocator doesn't exist.
			// We already achieved the goal of releasing the SID, so we don't
			// need to return an error.
			return nil
		}

		if err := allocator.Release(sid.Addr); err != nil {
			return err
		}

		return nil
	}
}

// createIngressPathVRFs will range over the provided VRFs and configure
// the datapath for ingressing VPN traffic destined for this node's VRF.
//
// The ingress path configuration consists of the following for newly exported VRFs.
// 1. Allocating a SID for the VRF if necessary
// 2. Writing this SID and its associated VRF ID to the SRv6SIDMap //TODO: checking spelling
// 3. Store the allocated SID wihin the Manager's memory.
func (m *Manager) createIngressPathVRFs(vrfs []*VRF) {
	l := m.logger.WithFields(
		logrus.Fields{
			"component": "srv6.Manager.createIngressPathVRFs",
		},
	)
	for _, vrf := range vrfs {
		func(vrf *VRF) {
			// ATTENTION: variables declared here so cleanup function can close
			// over them, do not redeclare these vars.
			var (
				err  error
				info *sidmanager.SIDInfo
			)

			// allocate a SID and defer possible cleanup.
			info, err = m.allocateSID(vrf.LocatorPool, vrf.id.Name)
			if err != nil {
				l := l.WithFields(logrus.Fields{
					"vrf":         vrf.id.Name,
					"locatorPool": vrf.LocatorPool,
				})
				if errors.Is(err, sidmanager.ErrAllocatorNotFound) {
					// Allocator not ready yet, nothing to reconcile for now.
					// This is expected e.g. if the VRF is deployed earlier than the referenced locator pool.
					l.Debug("SID Allocator not ready yet")
				} else {
					l.WithError(err).Error("Failed to allocate SID for VRF")
				}
				return
			}
			defer func() {
				if err != nil {
					if err := m.releaseSID(vrf.LocatorPool, info.SID); err != nil {
						l.WithError(err).Errorf("Failed to cleanup SID Allocation %s", info.SID.String())
					}
				}
			}()

			// populate SID map
			if err := m.sidMap.Update(&srv6map.SIDKey{SID: info.SID.As16()}, vrf.VRFID); err != nil {
				l.WithField("vrf", vrf.id.Name).WithError(err).Error("Failed to update SID Map")
				return
			}

			m.updateVRFSIDAllocation(vrf, vrf.LocatorPool, info)

			l.WithFields(logrus.Fields{
				"VRF":         vrf.id.Name,
				"LocatorPool": vrf.LocatorPool,
				"SID":         vrf.SIDInfo.SID.String(),
			}).Info("Allocated SID for VRF with export route target.")
		}(vrf)
	}
}

func (m *Manager) updateIngressPathVRFs(vrfs []*VRF) {
	l := m.logger.WithFields(
		logrus.Fields{
			"component": "srv6.Manager.updateIngressPaths",
		},
	)
	for _, vrf := range vrfs {
		func() {
			var (
				err     error
				newInfo *sidmanager.SIDInfo
			)

			l := l.WithField("vrf", vrf.id.Name)

			oldAllocation, ok := m.allocatedSIDs[vrf.VRFID]
			if !ok {
				l.Error("Failed to retrieve old SID")
				return
			}

			newInfo, err = m.allocateSID(vrf.LocatorPool, vrf.id.Name)
			if err != nil {
				l.WithError(err).Error("Failed to allocate new SID")
				return
			}

			defer func() {
				if err != nil {
					if err := m.releaseSID(vrf.LocatorPool, newInfo.SID); err != nil {
						l.WithError(err).Error("Failed to recover by releasing new SID")
					}
				}
			}()

			err = m.sidMap.Delete(&srv6map.SIDKey{SID: oldAllocation.SIDInfo.SID.As16()})
			if err != nil && !errors.Is(err, ebpf.ErrKeyNotExist) {
				l.WithError(err).Error("Failed to delete SID map entry")
				return
			}

			defer func() {
				if err != nil {
					if err := m.sidMap.Update(&srv6map.SIDKey{SID: oldAllocation.SIDInfo.SID.As16()}, vrf.VRFID); err != nil {
						l.WithError(err).Error("Failed to recover by updating SID Map with old SID")
					}
				}
			}()

			err = m.releaseSID(oldAllocation.LocatorPool, oldAllocation.SIDInfo.SID)
			if err != nil {
				l.WithError(err).Error("failed to release old SID")
				return
			}

			err = m.sidMap.Update(&srv6map.SIDKey{SID: newInfo.SID.As16()}, vrf.VRFID)
			if err != nil {
				l.WithError(err).Error("Failed to update SID map entry")
				return
			}

			defer func() {
				if err != nil {
					if err := m.sidMap.Delete(&srv6map.SIDKey{SID: newInfo.SID.As16()}); err != nil {
						l.WithError(err).Error("Failed to recover by deleting new SID from SID Map")
					}
				}
			}()

			m.deleteVRFSIDAllocation(oldAllocation.VRFID)
			m.updateVRFSIDAllocation(vrf, vrf.LocatorPool, newInfo)

			l.WithFields(logrus.Fields{
				"VRF":         vrf.id.Name,
				"LocatorPool": vrf.LocatorPool,
				"SID":         vrf.SIDInfo.SID.String(),
			}).Info("Updated SID for VRF")
		}()
	}
}

// removeIngressPathVRFs ranges over the provided SIDAllocation(s) and
// removes their existence from the data path.
//
// this is essentially the opposite of createIngressPathVRF.
//
// if an error occurs in any of the operations involved with removing a SID
// allocation the removal will be tried again on next reconciliation.
func (m *Manager) removeIngressPathVRFs(allocs []*SIDAllocation) {
	l := m.logger.WithFields(
		logrus.Fields{
			"component": "srv6.Manager.removeIngressPaths",
		},
	)
	for _, alloc := range allocs {
		l := l.WithFields(
			logrus.Fields{
				"SID":   alloc.SIDInfo.SID.String(),
				"vrfID": alloc.VRFID,
			},
		)
		var shouldRelease = true
		if err := m.sidMap.Delete(&srv6map.SIDKey{SID: alloc.SIDInfo.SID.As16()}); err != nil && !errors.Is(err, ebpf.ErrKeyNotExist) {
			l.WithError(err).Error("failed deleting SIDMap entry for allocation")
			shouldRelease = false
		}
		if shouldRelease {
			m.releaseSID(alloc.LocatorPool, alloc.SIDInfo.SID)
			m.deleteVRFSIDAllocation(alloc.VRFID)
			l.Info("Deleted SID allocation for VRF")
		}
	}
}

// reconcilePoliciesAndSIDs is responsible for reconciling the state of the
// manager (i.e. the desired state) with the actual state of the node (SRv6
// policy map entries).
//
// Whenever it encounters an error, it will just log it and move to the next
// item, in order to reconcile as many states as possible.
func (manager *Manager) reconcilePoliciesAndSIDs() {
	// The order of the next 2 function calls matters, as by first adding missing policies and
	// only then removing obsolete ones we make sure there will be no connectivity disruption
	manager.addMissingSRv6PolicyRules()
	manager.removeUnusedSRv6PolicyRules()
}

// reconcileVRF is responsible for reconciling the state of the
// manager (i.e. the desired state) with the actual state of the node (SRv6
// VRF mapping maps).
//
// Whenever it encounters an error, it will just log it and move to the next
// item, in order to reconcile as many states as possible.
func (manager *Manager) reconcileVRF() {
	l := manager.logger.WithFields(
		logrus.Fields{
			"component": "srv6.Manager.reconcileVRF",
		},
	)

	if manager.sidAllocatorIsSet() {
		manager.reconcileVRFIngressPath()
	} else {
		l.Debug("SRv6 Manager not configured with SID Allocator yet, won't export VRFs.")
	}

	manager.reconcileVRFEgressPath()

	manager.bgp.Event(struct{}{})
}
