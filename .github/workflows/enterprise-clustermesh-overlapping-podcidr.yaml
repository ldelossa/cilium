name: ClusterMesh with Overlapping PodCIDR (ci-clustermesh)

# Any change in triggers needs to be reflected in the concurrency group.
on:
  workflow_dispatch:
    inputs:
      PR-number:
        description: "Pull request number."
        required: true
      context-ref:
        description: "Context in which the workflow runs. If PR is from a fork, will be the PR target branch (general case). If PR is NOT from a fork, will be the PR branch itself (this allows committers to test changes to workflows directly from PRs)."
        required: true
      SHA:
        description: "SHA under test (head of the PR branch)."
        required: true
      extra-args:
        description: "[JSON object] Arbitrary arguments passed from the trigger comment via regex capture group. Parse with 'fromJson(inputs.extra-args).argName' in workflow."
        required: false
        default: '{}'
  # Run every 6 hours
  schedule:
    - cron:  '0 3/6 * * *'

# By specifying the access of one of the scopes, all of those that are not
# specified are set to 'none'.
permissions:
  # Required by the telemetry collection step
  actions: read
  # To be able to access the repository with actions/checkout
  contents: read
  # To allow retrieving information from the PR API
  pull-requests: read
  # To be able to set commit status
  statuses: write

concurrency:
  # Structure:
  # - Workflow name
  # - Event type
  # - A unique identifier depending on event type:
  #   - schedule: SHA
  #   - workflow_dispatch: PR number
  #
  # This structure ensures a unique concurrency group name is generated for each
  # type of testing, such that re-runs will cancel the previous run.
  group: |
    ${{ github.workflow }}
    ${{ github.event_name }}
    ${{
      (github.event_name == 'schedule' && github.sha) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.PR-number)
    }}
  cancel-in-progress: true

env:
  clusterName1: cluster1-${{ github.run_id }}
  clusterName2: cluster2-${{ github.run_id }}
  contextName1: kind-cluster1-${{ github.run_id }}
  contextName2: kind-cluster2-${{ github.run_id }}

jobs:
  commit-status-start:
    name: Commit Status Start
    runs-on: ubuntu-latest
    steps:
      - name: Set initial commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}

  installation-and-connectivity:
    name: Setup & Test
    runs-on: ${{ vars.GH_RUNNER_EXTRA_POWER }}
    timeout-minutes: 60

    steps:
    - name: Collect Workflow Telemetry
      uses: catchpoint/workflow-telemetry-action@94c3c3d9567a0205de6da68a76c428ce4e769af1 # v2.0.0
      with:
        comment_on_pr: false

    - name: Checkout context ref
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
      with:
        ref: ${{ inputs.context-ref || github.sha }}
        persist-credentials: false

    - name: Set Environment Variables
      uses: ./.github/actions/set-env-variables

    - name: Get Cilium's default values
      id: default_vars
      uses: ./.github/actions/helm-default
      with:
        image-tag: ${{ inputs.SHA }}
        chart-dir: ./untrusted/install/kubernetes/cilium

    - name: Set up job variables for GHA environment
      id: vars
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          SHA="${{ inputs.SHA }}"
        else
          SHA="${{ github.sha }}"
        fi

        # bpf.masquerade is explicitly disabled due to https://github.com/cilium/cilium/issues/23283#issuecomment-1597282247
        CILIUM_INSTALL_DEFAULTS="${{ steps.default_vars.outputs.cilium_install_defaults }} \
          --helm-set=bpf.masquerade=false \
          --helm-set=clustermesh.useAPIServer=true \
          --helm-set=enterprise.clustermesh.enableOverlappingPodCIDRSupport=true \
          --helm-set=socketLB.enabled=true \
          --helm-set=socketLB.hostNamespaceOnly=true \
          --helm-set=endpointHealthChecking.enabled=false \
          --helm-set=routingMode=tunnel \
          --helm-set=tunnelProtocol=vxlan \
        "

        CILIUM_INSTALL_IPFAMILY="--helm-set=ipv4.enabled=true --helm-set=ipv6.enabled=false"
        KIND_POD_CIDR_1="10.242.0.0/16"
        KIND_SVC_CIDR_1="10.243.0.0/16"
        KIND_POD_CIDR_2="10.242.0.0/16"
        KIND_SVC_CIDR_2="10.243.0.0/16"

        CONNECTIVITY_TEST_DEFAULTS="--hubble=false --flow-validation=disabled --external-target=google.com. --collect-sysdump-on-failure"

        echo cilium_install_defaults="${CILIUM_INSTALL_DEFAULTS} ${CILIUM_INSTALL_IPFAMILY}" >> $GITHUB_OUTPUT
        echo connectivity_test_defaults=${CONNECTIVITY_TEST_DEFAULTS} >> $GITHUB_OUTPUT

        echo kind_pod_cidr_1=${KIND_POD_CIDR_1} >> $GITHUB_OUTPUT
        echo kind_svc_cidr_1=${KIND_SVC_CIDR_1} >> $GITHUB_OUTPUT
        echo kind_pod_cidr_2=${KIND_POD_CIDR_2} >> $GITHUB_OUTPUT
        echo kind_svc_cidr_2=${KIND_SVC_CIDR_2} >> $GITHUB_OUTPUT

        echo sha=${SHA} >> $GITHUB_OUTPUT

    - name: Install Cilium CLI
      uses: cilium/cilium-cli@fff38e882846c03f1720dad476e459323275ab9c # v0.16.17
      with:
        skip-build: ${{ env.CILIUM_CLI_SKIP_BUILD }}
        image-repo: ${{ env.CILIUM_CLI_IMAGE_REPO }}
        image-tag: ${{ steps.vars.outputs.sha }}

    - name: Generate Kind configuration files
      run: |
        PODCIDR=${{ steps.vars.outputs.kind_pod_cidr_1 }} \
          SVCCIDR=${{ steps.vars.outputs.kind_svc_cidr_1 }} \
          IPFAMILY=ipv4 \
          KUBEPROXYMODE=none \
          envsubst < ./.github/kind-config.yaml.tmpl > ./.github/kind-config-cluster1.yaml

        PODCIDR=${{ steps.vars.outputs.kind_pod_cidr_2 }} \
          SVCCIDR=${{ steps.vars.outputs.kind_svc_cidr_2 }} \
          IPFAMILY=ipv4 \
          KUBEPROXYMODE=none \
          envsubst < ./.github/kind-config.yaml.tmpl > ./.github/kind-config-cluster2.yaml

    - name: Create Kind cluster 1
      uses: helm/kind-action@0025e74a8c7512023d06dc019c617aa3cf561fde # v1.10.0
      with:
        cluster_name: ${{ env.clusterName1 }}
        version: ${{ env.KIND_VERSION }}
        node_image: ${{ env.KIND_K8S_IMAGE }}
        kubectl_version: ${{ env.KIND_K8S_VERSION }}
        config: ./.github/kind-config-cluster1.yaml
        wait: 0 # The control-plane never becomes ready, since no CNI is present

    - name: Create Kind cluster 2
      uses: helm/kind-action@0025e74a8c7512023d06dc019c617aa3cf561fde # v1.10.0
      with:
        cluster_name: ${{ env.clusterName2 }}
        version: ${{ env.KIND_VERSION }}
        node_image: ${{ env.KIND_K8S_IMAGE }}
        kubectl_version: ${{ env.KIND_K8S_VERSION }}
        config: ./.github/kind-config-cluster2.yaml
        wait: 0 # The control-plane never becomes ready, since no CNI is present

    - name: Wait for images to be available
      timeout-minutes: 20
      shell: bash
      run: |
        for image in cilium-ci operator-generic-ci hubble-relay-ci clustermesh-apiserver-ci ; do
          until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.vars.outputs.sha }} &> /dev/null; do sleep 45s; done
        done

    # Warning: since this is a privileged workflow, subsequent workflow job
    # steps must take care not to execute untrusted code.
    - name: Checkout pull request branch (NOT TRUSTED)
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
      with:
        ref: ${{ steps.vars.outputs.sha }}
        persist-credentials: false
        path: untrusted
        sparse-checkout: |
          install/kubernetes/cilium

    - name: Install Cilium in cluster1
      run: |
        # Explicitly configure the NodePort to make sure that it is different in
        # each cluster, to workaround #24692
        cilium --context ${{ env.contextName1 }} install \
        ${{ steps.vars.outputs.cilium_install_defaults }} \
          --helm-set cluster.name=${{ env.clusterName1 }} \
          --helm-set cluster.id=1 \
          --helm-set clustermesh.apiserver.service.nodePort=32379

    - name: Copy the Cilium CA secret to cluster2, as they must match
      run: |
        kubectl --context ${{ env.contextName1 }} get secret -n kube-system cilium-ca -o yaml |
          kubectl --context ${{ env.contextName2 }} create -f -

    - name: Install Cilium in cluster2
      run: |
        # Explicitly configure the NodePort to make sure that it is different in
        # each cluster, to workaround #24692
        cilium --context ${{ env.contextName2 }} install \
          ${{ steps.vars.outputs.cilium_install_defaults }} \
          --helm-set cluster.name=${{ env.clusterName2 }} \
          --helm-set cluster.id=255 \
          --helm-set clustermesh.apiserver.service.nodePort=32380

    - name: Wait for cluster mesh status to be ready
      run: |
        cilium --context ${{ env.contextName1 }} status --wait
        cilium --context ${{ env.contextName2 }} status --wait
        cilium --context ${{ env.contextName1 }} clustermesh status --wait
        cilium --context ${{ env.contextName2 }} clustermesh status --wait

    - name: Connect clusters
      run: |
        cilium --context ${{ env.contextName1 }} clustermesh connect --destination-context ${{ env.contextName2 }}

    - name: Wait for cluster mesh status to be ready
      run: |
        cilium --context ${{ env.contextName1 }} status --wait
        cilium --context ${{ env.contextName2 }} status --wait
        cilium --context ${{ env.contextName1 }} clustermesh status --wait
        cilium --context ${{ env.contextName2 }} clustermesh status --wait

    - name: Deploy required resources
      run: |
        cat << EOF > netshoot.yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: netshoot
          labels:
            app: netshoot
        spec:
          nodeName: ${{ env.clusterName1 }}-worker
          containers:
          - name: netshoot
            image: nicolaka/netshoot:v0.9
            command: ["sleep", "infinite"]
        EOF
        cat << EOF > httpbin.yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: httpbin
          labels:
            app: httpbin
        spec:
          nodeName: ${{ env.clusterName2 }}-worker
          containers:
          - name: httpbin
            image: kennethreitz/httpbin
        EOF
        cat << EOF > httpbin-service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: httpbin-service
          annotations:
            service.cilium.io/global: "true"
        spec:
          type: ClusterIP
          selector:
            app: httpbin
          ports:
            - protocol: TCP
              port: 80
              targetPort: 80
        EOF
        cat << EOF > network-policy-egress.yaml
        apiVersion: "cilium.io/v2"
        kind: CiliumNetworkPolicy
        metadata:
          name: "allow-cross-cluster-egress"
        spec:
          endpointSelector:
            matchLabels:
              app: netshoot
              io.cilium.k8s.policy.cluster: ${{ env.clusterName1 }}
          egress:
          # Allow inter-cluster communication with httpbin service
          - toEndpoints:
            - matchLabels:
                app: httpbin
                io.cilium.k8s.policy.cluster: ${{ env.clusterName2 }}
            toPorts:
            - ports:
              - port: "80"
                protocol: TCP
          # Allow name resolution
          - toEndpoints:
            - matchLabels:
                k8s-app: kube-dns
                io.kubernetes.pod.namespace: kube-system
            toPorts:
            - ports:
              - port: "53"
                protocol: ANY
        EOF
        cat << EOF > network-policy-ingress.yaml
        apiVersion: "cilium.io/v2"
        kind: CiliumNetworkPolicy
        metadata:
          name: "allow-cross-cluster-ingress"
        spec:
          endpointSelector:
            matchLabels:
              app: httpbin
              io.cilium.k8s.policy.cluster: ${{ env.clusterName2 }}
          ingress:
          - fromEndpoints:
            - matchLabels:
                app: netshoot
                io.cilium.k8s.policy.cluster: ${{ env.clusterName1 }}
        EOF

        kubectl --context ${{ env.contextName1 }} apply -f netshoot.yaml
        kubectl --context ${{ env.contextName2 }} apply -f httpbin.yaml
        kubectl --context ${{ env.contextName1 }} apply -f httpbin-service.yaml
        kubectl --context ${{ env.contextName2 }} apply -f httpbin-service.yaml
        kubectl --context ${{ env.contextName1 }} apply -f network-policy-egress.yaml
        kubectl --context ${{ env.contextName2 }} apply -f network-policy-ingress.yaml
        kubectl --context ${{ env.contextName1 }} wait pods -l app=netshoot --for condition=Ready --timeout=300s
        kubectl --context ${{ env.contextName2 }} wait pods -l app=httpbin --for condition=Ready --timeout=300s

        wait_for_policy_status() {
          local pod=$1 context=$2 expected=$3
          local epid node agent got

          epid=$(kubectl --context $context get cep $pod -o jsonpath='{.status.id}')
          node=$(kubectl --context $context get pod $pod -o jsonpath='{.spec.nodeName}')
          agent=$(kubectl --context $context get pod -n kube-system -o jsonpath='{.items[].metadata.name}' \
              -l app.kubernetes.io/name=cilium-agent --field-selector spec.nodeName=$node)

          for i in {1..60}; do
              got=$(kubectl --context $context exec -n kube-system $agent -c cilium-agent -- \
                  cilium endpoint get $epid -o jsonpath='{$[0].status.policy.realized.policy-enabled}')

              echo "Policy status for cep is $got, expected $expected ($i attempt)"
              if [ "$got" == "$expected" ]; then return 0; fi
              sleep 1
          done

          echo "Timeout waiting for policy status to match the expected value"
          return 1
        }

        wait_for_policy_status netshoot ${{ env.contextName1 }} egress
        wait_for_policy_status httpbin ${{ env.contextName2 }} ingress

    - name: Ensure inter-cluster communication over global service with L3/L4 policy is working correctly
      run: |
        # Source address of the request should be an IP of the node that the source Pod is running on
        EXPECT=$(docker inspect ${{ env.clusterName1 }}-worker | jq -r '.[0].NetworkSettings.Networks["kind"].IPAddress')

        # Make a request to service and get origin IP address returned from server
        GOT=$(kubectl --context ${{ env.contextName1 }} exec -it netshoot -- curl -s http://httpbin-service.default.svc.cluster.local/get | jq -r .origin)

        # Check if the source address is an expected one or not. If it is expected one that means...
        # 1. Basic connectivity over global service is working
        # 2. Egress network policy was correctly applied
        # 3. Inter-cluster SNAT happened
        # 4. Ingress network policy was correctly applied even if we lose the real source IP
        if [ $GOT != $EXPECT ]; then
          echo "Connectivity test failed. Expect: ${EXPECT}, Got: ${GOT}"
          exit 1
        fi

    - name: Ensure intra-cluster communication is working as is
      run: |
        cilium --context ${{ env.contextName1 }} connectivity test ${{ steps.vars.outputs.connectivity_test_defaults }}

    - name: Post-test information gathering
      if: ${{ !success() }}
      run: |
        cilium --context ${{ env.contextName1 }} status
        cilium --context ${{ env.contextName1 }} clustermesh status
        cilium --context ${{ env.contextName2 }} status
        cilium --context ${{ env.contextName2 }} clustermesh status

        kubectl config use-context ${{ env.contextName1 }}
        kubectl get pods --all-namespaces -o wide
        cilium sysdump --output-filename cilium-sysdump-context1-final

        kubectl config use-context ${{ env.contextName2 }}
        kubectl get pods --all-namespaces -o wide
        cilium sysdump --output-filename cilium-sysdump-context2-final
      shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

    - name: Upload artifacts
      if: ${{ !success() }}
      uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
      with:
        name: cilium-sysdumps
        path: cilium-sysdump-*.zip
        retention-days: 5

  commit-status-final:
    name: Commit Status Final
    if: ${{ always() }}
    needs: installation-and-connectivity
    runs-on: ubuntu-latest
    steps:
      - name: Set final commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}
          status: ${{ needs.installation-and-connectivity.result }}
