name: CiliumMesh (ci-cilium-mesh)

# Any change in triggers needs to be reflected in the concurrency group.
on:
  workflow_dispatch:
    inputs:
      PR-number:
        description: "Pull request number."
        required: true
      context-ref:
        description: "Context in which the workflow runs. If PR is from a fork, will be the PR target branch (general case). If PR is NOT from a fork, will be the PR branch itself (this allows committers to test changes to workflows directly from PRs)."
        required: true
      SHA:
        description: "SHA under test (head of the PR branch)."
        required: true
      extra-args:
        description: "[JSON object] Arbitrary arguments passed from the trigger comment via regex capture group. Parse with 'fromJson(inputs.extra-args).argName' in workflow."
        required: false
        default: '{}'
  # Run every 6 hours
  schedule:
    - cron:  '0 3/6 * * *'

# By specifying the access of one of the scopes, all of those that are not
# specified are set to 'none'.
permissions:
  # Required by the telemetry collection step
  actions: read
  # To be able to access the repository with actions/checkout
  contents: read
  # To allow retrieving information from the PR API
  pull-requests: read
  # To be able to set commit status
  statuses: write

concurrency:
  # Structure:
  # - Workflow name
  # - Event type
  # - A unique identifier depending on event type:
  #   - schedule: SHA
  #   - workflow_dispatch: PR number
  #
  # This structure ensures a unique concurrency group name is generated for each
  # type of testing, such that re-runs will cancel the previous run.
  group: |
    ${{ github.workflow }}
    ${{ github.event_name }}
    ${{
      (github.event_name == 'schedule' && github.sha) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.PR-number)
    }}
  cancel-in-progress: true

env:
  # renovate: datasource=github-releases depName=kubernetes-sigs/kind
  kind_version: v0.24.0
  k8s_version: v1.27.1
  clusterName1: cluster1-${{ github.run_id }}
  clusterName2: cluster2-${{ github.run_id }}
  contextName1: kind-cluster1-${{ github.run_id }}
  contextName2: kind-cluster2-${{ github.run_id }}
  check_url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

jobs:
  commit-status-start:
    name: Commit Status Start
    runs-on: ubuntu-latest
    steps:
      - name: Set initial commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}

  installation-and-connectivity:
    name: "Installation and Connectivity Test"
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      job_name: "Installation and Connectivity Test"

    strategy:
      fail-fast: false
      matrix:
        include:
          - name: '1'
            ipfamily: 'ipv4'
            encryption: 'disabled'
            kube-proxy: 'none'
            kvstoremesh: false
            cm-auth-mode-1: 'legacy'
            cm-auth-mode-2: 'legacy'

    steps:
      - name: Collect Workflow Telemetry
        uses: catchpoint/workflow-telemetry-action@94c3c3d9567a0205de6da68a76c428ce4e769af1 # v2.0.0
        with:
          comment_on_pr: false

      - name: Checkout context ref
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
        with:
          ref: ${{ inputs.context-ref || github.sha }}
          persist-credentials: false

      - name: Set Environment Variables
        uses: ./.github/actions/set-env-variables

      - name: Set up job variables for GHA environment
        id: vars
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SHA="${{ inputs.SHA }}"
          else
            SHA="${{ github.sha }}"
          fi

          # XXX fix the SHA for now so that I don't have to wait for image build
          SHASHA=${SHA}
          #SHASHA=b03a71925b1fe6aed15b50daf370a87b41c3c869

          CILIUM_INSTALL_DEFAULTS="--chart-directory=install/kubernetes/cilium \
            --helm-set=debug.enabled=true \
            --helm-set=image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/cilium-ci \
            --helm-set=image.useDigest=false \
            --helm-set=image.tag=${SHASHA} \
            --helm-set=operator.image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/operator \
            --helm-set=operator.image.suffix="-ci" \
            --helm-set=operator.image.tag=${SHASHA} \
            --helm-set=operator.image.useDigest=false \
            --helm-set=ipam.mode=kubernetes \
            --helm-set=bpf.masquerade=true \
            --helm-set=bpf.monitorAggregation=none \
            --helm-set=bpf.lbExternalClusterIP=true \
            --helm-set=socketLB.enabled=true \
            --helm-set=socketLB.hostNamespaceOnly=true \
            --helm-set=endpointHealthChecking.enabled=false \
            --helm-set=devices=eth+ \
            --helm-set=hubble.enabled=true \
            --helm-set=hubble.relay.enabled=true \
            --helm-set=hubble.relay.image.override=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/hubble-relay-ci:${SHASHA} \
            --helm-set=hubble.relay.image.useDigest=false \
            --helm-set=clustermesh.useAPIServer=true \
            --helm-set=clustermesh.apiserver.kvstoremesh.enabled=${{ matrix.kvstoremesh }} \
            --helm-set=clustermesh.apiserver.image.override=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/clustermesh-apiserver-ci:${SHASHA} \
            --helm-set=clustermesh.apiserver.image.useDigest=false \
            --helm-set=clustermesh.apiserver.kvstoremesh.image.override=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/kvstoremesh-ci:${SHASHA} \
            --helm-set=clustermesh.apiserver.kvstoremesh.image.useDigest=false \
            --helm-set=clustermesh.config.enabled=true \
            --helm-set=clustermesh.config.enableClusterAwareAddressing=true \
            --helm-set=clustermesh.config.hasOverlappingPodCIDR=true \
            --helm-set enterprise.clustermesh.enableOverlappingPodCIDRSupport=true \
            --helm-set=enterprise.ciliummesh.enabled=true \
            --helm-set=routingMode=tunnel \
            --helm-set=tunnelProtocol=vxlan \
            --helm-set=autoDirectNodeRoutes=false \
            --helm-set=kubeProxyReplacement=true \
            --helm-set=k8s.requireIPv4PodCIDR=true \
            --helm-set=k8s.requireIPv6PodCIDR=false \
            "

          case "${{ matrix.ipfamily }}" in
            ipv4)
              CILIUM_INSTALL_IPFAMILY="--helm-set=ipv4.enabled=true --helm-set=ipv6.enabled=false"
              KIND_POD_CIDR_1="10.242.0.0/16"
              KIND_SVC_CIDR_1="10.243.0.0/16"
              KIND_POD_CIDR_2="10.244.0.0/16"
              KIND_SVC_CIDR_2="10.245.0.0/16"
              ;;
            ipv6)
              CILIUM_INSTALL_IPFAMILY="--helm-set=ipv4.enabled=false --helm-set=ipv6.enabled=true"
              KIND_POD_CIDR_1="fd00:10:242::/48"
              KIND_SVC_CIDR_1="fd00:10:243::/112"
              KIND_POD_CIDR_2="fd00:10:244::/48"
              KIND_SVC_CIDR_2="fd00:10:245::/112"
              ;;
            dual)
              CILIUM_INSTALL_IPFAMILY="--helm-set=ipv4.enabled=true --helm-set=ipv6.enabled=true"
              KIND_POD_CIDR_1="10.242.0.0/16,fd00:10:242::/48"
              KIND_SVC_CIDR_1="10.243.0.0/16,fd00:10:243::/112"
              KIND_POD_CIDR_2="10.244.0.0/16,fd00:10:244::/48"
              KIND_SVC_CIDR_2="10.245.0.0/16,fd00:10:245::/112"
              ;;
            *)
              echo "Unknown IP family '${{ matrix.ipfamily }}'" && false
              ;;
          esac

          echo cilium_install_defaults="${CILIUM_INSTALL_DEFAULTS} ${CILIUM_INSTALL_IPFAMILY}" >> $GITHUB_OUTPUT
          echo sha=${SHA} >> $GITHUB_OUTPUT

          echo kind_pod_cidr_1=${KIND_POD_CIDR_1} >> $GITHUB_OUTPUT
          echo kind_svc_cidr_1=${KIND_SVC_CIDR_1} >> $GITHUB_OUTPUT
          echo kind_pod_cidr_2=${KIND_POD_CIDR_2} >> $GITHUB_OUTPUT
          echo kind_svc_cidr_2=${KIND_SVC_CIDR_2} >> $GITHUB_OUTPUT

      - name: Install Cilium CLI
        uses: cilium/cilium-cli@62bd4511031211b50a4623870955a5ad27b43e3b # v0.16.16
        with:
          skip-build: ${{ env.CILIUM_CLI_SKIP_BUILD }}
          image-repo: ${{ env.CILIUM_CLI_IMAGE_REPO }}
          image-tag: ${{ steps.vars.outputs.sha }}

      - name: Generate Kind configuration files
        run: |
          K8S_VERSION=${{ env.k8s_version }} \
            PODCIDR=${{ steps.vars.outputs.kind_pod_cidr_1 }} \
            SVCCIDR=${{ steps.vars.outputs.kind_svc_cidr_1 }} \
            IPFAMILY=${{ matrix.ipfamily }} \
            KUBEPROXYMODE=${{ matrix.kube-proxy }} \
            envsubst < ./.github/enterprise-cilium-mesh/kind-config.yaml.tmpl > ./.github/kind-config-cluster1.yaml

          K8S_VERSION=${{ env.k8s_version }} \
            PODCIDR=${{ steps.vars.outputs.kind_pod_cidr_2 }} \
            SVCCIDR=${{ steps.vars.outputs.kind_svc_cidr_2 }} \
            IPFAMILY=${{ matrix.ipfamily }} \
            KUBEPROXYMODE=${{ matrix.kube-proxy }} \
            envsubst < ./.github/enterprise-cilium-mesh/kind-config.yaml.tmpl > ./.github/kind-config-cluster2.yaml

      - name: Create Kind cluster 1
        uses: helm/kind-action@0025e74a8c7512023d06dc019c617aa3cf561fde # v1.10.0
        with:
          cluster_name: ${{ env.clusterName1 }}
          version: ${{ env.kind_version }}
          kubectl_version: ${{ env.k8s_version }}
          config: ./.github/kind-config-cluster1.yaml
          wait: 0 # The control-plane never becomes ready, since no CNI is present

      - name: Create Kind cluster 2
        uses: helm/kind-action@0025e74a8c7512023d06dc019c617aa3cf561fde # v1.10.0
        with:
          cluster_name: ${{ env.clusterName2 }}
          version: ${{ env.kind_version }}
          kubectl_version: ${{ env.k8s_version }}
          config: ./.github/kind-config-cluster2.yaml
          wait: 0 # The control-plane never becomes ready, since no CNI is present

      # Make sure that coredns uses IPv4-only upstream DNS servers also in case of clusters
      # with IP family dual, since IPv6 ones are not reachable and cause spurious failures.
      - name: Configure the coredns nameservers
        if: matrix.ipfamily == 'dual'
        run: |
          COREDNS_PATCH="
          spec:
            template:
              spec:
                dnsPolicy: None
                dnsConfig:
                  nameservers:
                  - 8.8.4.4
                  - 8.8.8.8
          "

          kubectl --context ${{ env.contextName1 }} patch deployment -n kube-system coredns --patch="$COREDNS_PATCH"
          kubectl --context ${{ env.contextName2 }} patch deployment -n kube-system coredns --patch="$COREDNS_PATCH"

      - name: Wait for images to be available
        timeout-minutes: 10
        shell: bash
        run: |
          for image in cilium-ci operator-generic-ci hubble-relay-ci clustermesh-apiserver-ci ; do
            until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.vars.outputs.sha }} &> /dev/null; do sleep 45s; done
          done

      # We need to checkout the SHA to retrieve the Helm chart Warning: since
      # this is a privileged workflow, we should be careful NOT to use anything
      # coming from an external contributor in a privileged environment. Here
      # it's fine because we pass the Helm chart to be installed in a
      # Kubernetes cluster, so it won't have access to the privileged
      # environment from there.
      - name: Checkout SHA
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
        with:
          ref: ${{ steps.vars.outputs.sha }}
          persist-credentials: false

      - name: Install Cilium in cluster1
        id: install-cilium-cluster1
        run: |
          # Explicitly configure the NodePort to make sure that it is different in
          # each cluster, to workaround #24692
          cilium --context ${{ env.contextName1 }} install \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            --helm-set cluster.name=${{ env.clusterName1 }} \
            --helm-set cluster.id=1 \
            --helm-set clustermesh.apiserver.service.nodePort=32379 \
            --helm-set clustermesh.apiserver.tls.authMode=${{ matrix.cm-auth-mode-1 }}

      - name: Copy the Cilium CA secret to cluster2, as they must match
        run: |
          kubectl --context ${{ env.contextName1 }} get secret -n kube-system cilium-ca -o yaml |
            kubectl --context ${{ env.contextName2 }} create -f -

      - name: Install Cilium in cluster2
        run: |
          # Explicitly configure the NodePort to make sure that it is different in
          # each cluster, to workaround #24692
          cilium --context ${{ env.contextName2 }} install \
            ${{ steps.vars.outputs.cilium_install_defaults }} \
            --helm-set cluster.name=${{ env.clusterName2 }} \
            --helm-set cluster.id=2 \
            --helm-set clustermesh.apiserver.service.nodePort=32380 \
            --helm-set clustermesh.apiserver.tls.authMode=${{ matrix.cm-auth-mode-2 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait
          cilium --context ${{ env.contextName2 }} status --wait
          cilium --context ${{ env.contextName1 }} clustermesh status --wait
          cilium --context ${{ env.contextName2 }} clustermesh status --wait

      - name: Connect clusters
        run: |
          cilium --context ${{ env.contextName1 }} clustermesh connect --destination-context ${{ env.contextName2 }}

      - name: Wait for cluster mesh status to be ready
        run: |
          cilium --context ${{ env.contextName1 }} status --wait
          cilium --context ${{ env.contextName2 }} status --wait
          cilium --context ${{ env.contextName1 }} clustermesh status --wait
          cilium --context ${{ env.contextName2 }} clustermesh status --wait

      - name: Port forward Relay
        run: |
          cilium --context ${{ env.contextName1 }} hubble port-forward &
          sleep 10s
          [[ $(pgrep -f "cilium.*hubble.*port-forward|kubectl.*port-forward.*hubble-relay" | wc -l) == 2 ]]

      - name: Add Cilium Mesh endpoints
        run: |
          bash ./.github/enterprise-cilium-mesh/ep-add.sh "${{ env.contextName1 }}" "${{ env.clusterName1 }}" kind client ubuntu app=client "${{ steps.vars.outputs.kind_svc_cidr_1 }}"
          bash ./.github/enterprise-cilium-mesh/ep-add.sh "${{ env.contextName1 }}" "${{ env.clusterName1 }}" kind clientbad ubuntu app=clientbad "${{ steps.vars.outputs.kind_svc_cidr_1 }}"
          bash ./.github/enterprise-cilium-mesh/ep-add.sh "${{ env.contextName2 }}" "${{ env.clusterName2 }}" kind server1 nginx run=nginx "${{ steps.vars.outputs.kind_svc_cidr_2 }}"
          bash ./.github/enterprise-cilium-mesh/ep-add.sh "${{ env.contextName2 }}" "${{ env.clusterName2 }}" kind server2 nginx run=nginx "${{ steps.vars.outputs.kind_svc_cidr_2 }}"
          bash ./.github/enterprise-cilium-mesh/ep-add.sh "${{ env.contextName2 }}" "${{ env.clusterName2 }}" kind forbidden-fruit nginx run=forbidden-fruit "${{ steps.vars.outputs.kind_svc_cidr_2 }}"

      - name: Configure clusters
        run: |
          kubectl --context ${{ env.contextName1 }} apply -f ./.github/enterprise-cilium-mesh/cluster-01-config.yaml
          kubectl --context ${{ env.contextName2 }} apply -f ./.github/enterprise-cilium-mesh/cluster-02-config.yaml

          # smart synchronization
          sleep 2

      - name: Connectivity Test
        run: |
          die() {
                  echo "$@" >&2
                  exit 1
          }

          nginx_cluster_ip=$(kubectl --context "${{ env.contextName1 }}" get svc "nginx" -o json | jq -r '.spec.clusterIP')
          ff_cluster_ip=$(kubectl --context "${{ env.contextName1 }}" get svc "forbidden-fruit" -o json | jq -r '.spec.clusterIP')

          client_ep_id=$(kubectl -n kube-system --context ${{ env.contextName1 }} exec ds/cilium -- cilium endpoint list -o json |
                  tr -d '\n' | jq '.[].status.identity | select(.labels[] == "k8s:app=client") | .id' )
          clientbad_ep_id=$(kubectl -n kube-system --context ${{ env.contextName1 }} exec ds/cilium -- cilium endpoint list -o json |
                  tr -d '\n' | jq '.[].status.identity | select(.labels[] == "k8s:app=clientbad") | .id' )
          server_1_id=$(kubectl -n kube-system --context ${{ env.contextName2 }} exec ds/cilium -- cilium endpoint list -o json |
                  tr -d '\n' | jq '.[].status.identity | select(.labels[] == "k8s:name=server1") | .id' )
          server_2_id=$(kubectl -n kube-system --context ${{ env.contextName2 }} exec ds/cilium -- cilium endpoint list -o json |
                  tr -d '\n' | jq '.[].status.identity | select(.labels[] == "k8s:name=server2") | .id' )
          forbidden_fruit_id=$(kubectl -n kube-system --context ${{ env.contextName2 }} exec ds/cilium -- cilium endpoint list -o json |
                  tr -d '\n' | jq '.[].status.identity | select(.labels[] == "k8s:name=forbidden-fruit") | .id' )

          docker exec client curl --connect-timeout 10 ${nginx_cluster_ip} ||
                  die "client should have been able to connect to nginx=${nginx_cluster_ip}"
          kubectl --namespace kube-system --context ${{ env.contextName1 }} -c cilium-agent exec ds/cilium -- \
                  hubble observe --color never -t policy-verdict --from-identity ${client_ep_id} --to-identity ${server_1_id} --to-identity ${server_2_id} --last 1 | grep "EGRESS ALLOWED" ||
                  die "no allowed egress policy verdict in hubble flows"
          kubectl --namespace kube-system --context ${{ env.contextName2 }} -c cilium-agent exec ds/cilium -- \
                  hubble observe --color never -t policy-verdict --from-identity ${client_ep_id} --to-identity ${server_1_id} --to-identity ${server_2_id} --last 1 | grep "INGRESS ALLOWED" ||
                  die "no allowed ingress policy verdict in hubble flows"

          docker exec clientbad curl --connect-timeout 10 ${nginx_cluster_ip} &&
                  die "clientbad should not have been able to connect to nginx=${nginx_cluster_ip}"
          kubectl --namespace kube-system --context ${{ env.contextName2 }} -c cilium-agent exec ds/cilium -- \
                  hubble observe --color never -t policy-verdict --from-identity ${clientbad_ep_id} --to-identity ${server_1_id} --to-identity ${server_2_id} --last 1 | grep "INGRESS DENIED" ||
                  die "no denied ingress policy verdict in hubble flows"

          docker exec client     curl --connect-timeout 10 ${ff_cluster_ip}    &&
                  die "client should not have been able to connect to forbidden-fruit=${ff_cluster_ip}"
          kubectl --namespace kube-system --context ${{ env.contextName1 }} -c cilium-agent exec ds/cilium -- \
                  hubble observe --color never -t policy-verdict --from-identity ${client_ep_id} --to-identity ${forbidden_fruit_id} --last 1 | grep "EGRESS DENIED" ||
                  die "no denied egress policy verdict in hubble flows"

          docker exec clientbad curl --connect-timeout 10 ${ff_cluster_ip}    &&
                  die "clientbad should not have been able to connect to forbidden-fruit=${ff_cluster_ip}"
          kubectl --namespace kube-system --context ${{ env.contextName1 }} -c cilium-agent exec ds/cilium -- \
                  hubble observe --color never -t policy-verdict --from-identity ${clientbad_ep_id} --to-identity ${forbidden_fruit_id} --last 1 | grep "EGRESS DENIED" ||
                  die "no denied egress policy verdict in hubble flows"

      - name: Post-test information gathering
        if: ${{ !success() }}
        run: |
          cilium --context ${{ env.contextName1 }} status
          cilium --context ${{ env.contextName1 }} clustermesh status
          cilium --context ${{ env.contextName2 }} status
          cilium --context ${{ env.contextName2 }} clustermesh status

          kubectl config use-context ${{ env.contextName1 }}
          kubectl get pods --all-namespaces -o wide
          cilium sysdump --output-filename cilium-sysdump-context1-final-${{ join(matrix.*, '-') }}

          kubectl config use-context ${{ env.contextName2 }}
          kubectl get pods --all-namespaces -o wide
          cilium sysdump --output-filename cilium-sysdump-context2-final-${{ join(matrix.*, '-') }}
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Upload artifacts
        if: ${{ !success() }}
        uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: cilium-sysdumps
          path: cilium-sysdump-*.zip
          retention-days: 5

  commit-status-final:
    if: ${{ always() }}
    name: Commit Status Final
    needs: installation-and-connectivity
    runs-on: ubuntu-latest
    steps:
      - name: Set final commit status
        uses: myrotvorets/set-commit-status-action@3730c0a348a2ace3c110851bed53331bc6406e9f # v2.0.1
        with:
          sha: ${{ inputs.SHA || github.sha }}
          status: ${{ needs.installation-and-connectivity.result }}
